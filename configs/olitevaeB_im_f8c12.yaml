model:
  base_learning_rate: 1.0e-4
  target: networks.novel.olvae.models.liteautoencoder.LiteAutoencoderKL
  params:
    embed_dim: 12
    load_decoder: False
    use_quant: False
    ckpt_path: '/Users/talhaahmed/Research/Dr. Hassan RAship/Models/olitevaeB_im_f8c12.safetensors'

    encoder_config:
        target: networks.novel.olvae.modules.litevae.encoder_model.LiteVAE_Encoder
        params:
            # config for base liteVAE encoder. ~6.6M Params
            in_channels: 3
            dct_levels: 3
            latent_dim: 12
            image_size: 224
            # feature extractor (Note: there is one per DCT level)
            extractor_channels: 32
            extractor_mult: [1,2,3]
            extractor_resblocks: 4
            # feature aggregator
            aggregate_channels: 32
            aggregate_mult: [1, 2, 3]
            aggregate_resblocks: 4
            
    decoder_config:
        target: networks.novel.olvae.modules.litevae.decoder_model.LiteVAE_Decoder
        params:
            # config for SD-VAE's decoder. ~53M Params
            channels: 128
            z_channels: 12
            out_channels: 3 
            channel_mult: [1,2,4,4] 
            num_res_blocks: 2
        
    lossconfig:
      target: torch.nn.Identity
